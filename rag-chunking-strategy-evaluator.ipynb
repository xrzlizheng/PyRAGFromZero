{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "520d2427",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.005005,
     "end_time": "2025-05-26T16:49:27.505241",
     "exception": false,
     "start_time": "2025-05-26T16:49:27.500236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# RAG Chunking Strategy Evaluator\n",
    "\n",
    "A comprehensive system to evaluate and visualize different chunking strategies for RAG systems.\n",
    "Supports custom chunking strategies through inheritance and includes RAGAS evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f34915",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T16:49:27.515513Z",
     "iopub.status.busy": "2025-05-26T16:49:27.515160Z",
     "iopub.status.idle": "2025-05-26T16:49:27.718682Z",
     "shell.execute_reply": "2025-05-26T16:49:27.717675Z"
    },
    "papermill": {
     "duration": 0.210525,
     "end_time": "2025-05-26T16:49:27.720549",
     "exception": false,
     "start_time": "2025-05-26T16:49:27.510024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "os.environ['OPENAI_API_KEY'] = user_secrets.get_secret(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "333b7316",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T16:49:27.730371Z",
     "iopub.status.busy": "2025-05-26T16:49:27.730029Z",
     "iopub.status.idle": "2025-05-26T16:50:26.692722Z",
     "shell.execute_reply": "2025-05-26T16:50:26.691465Z"
    },
    "papermill": {
     "duration": 58.969805,
     "end_time": "2025-05-26T16:50:26.694772",
     "exception": false,
     "start_time": "2025-05-26T16:49:27.724967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.1/86.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.0/19.0 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.3/438.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.7/96.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\r\n",
      "google-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\r\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\r\n",
      "google-cloud-bigtable 2.30.0 requires google-api-core[grpc]<3.0.0,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\r\n",
      "pandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain-openai langchain-chroma ragas==0.1.9 plotly scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b0b4237",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-26T16:50:26.709700Z",
     "iopub.status.busy": "2025-05-26T16:50:26.709294Z",
     "iopub.status.idle": "2025-05-26T16:50:35.435361Z",
     "shell.execute_reply": "2025-05-26T16:50:35.434428Z"
    },
    "papermill": {
     "duration": 8.735742,
     "end_time": "2025-05-26T16:50:35.437175",
     "exception": false,
     "start_time": "2025-05-26T16:50:26.701433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/ragas/metrics/__init__.py:1: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.metrics._answer_correctness import AnswerCorrectness, answer_correctness\n",
      "/usr/local/lib/python3.11/dist-packages/ragas/metrics/__init__.py:4: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.metrics._context_entities_recall import (\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "import hashlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Core RAG components\n",
    "import chromadb\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# RAGAS evaluation\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    answer_correctness\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e2bb55b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-26T16:50:35.452073Z",
     "iopub.status.busy": "2025-05-26T16:50:35.451470Z",
     "iopub.status.idle": "2025-05-26T16:50:35.461357Z",
     "shell.execute_reply": "2025-05-26T16:50:35.460153Z"
    },
    "papermill": {
     "duration": 0.019213,
     "end_time": "2025-05-26T16:50:35.463094",
     "exception": false,
     "start_time": "2025-05-26T16:50:35.443881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ChunkingConfig:\n",
    "    \"\"\"Configuration for chunking strategies\"\"\"\n",
    "    name: str\n",
    "    chunk_size: int = 1000\n",
    "    chunk_overlap: int = 200\n",
    "    separators: Optional[List[str]] = None\n",
    "    custom_params: Optional[Dict[str, Any]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    \"\"\"Results from RAG evaluation\"\"\"\n",
    "    strategy_name: str\n",
    "    chunk_count: int\n",
    "    avg_chunk_size: float\n",
    "    retrieval_time: float\n",
    "    generation_time: float\n",
    "    answer_relevancy: float\n",
    "    faithfulness: float\n",
    "    context_precision: float\n",
    "    context_recall: float\n",
    "    answer_correctness: float\n",
    "    overall_score: float\n",
    "\n",
    "\n",
    "class ChunkingStrategy(ABC):\n",
    "    \"\"\"Abstract base class for chunking strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ChunkingConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    @abstractmethod\n",
    "    def chunk_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"Split documents into chunks\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return self.config.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea949d7b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-26T16:50:35.477456Z",
     "iopub.status.busy": "2025-05-26T16:50:35.477100Z",
     "iopub.status.idle": "2025-05-26T16:50:35.489609Z",
     "shell.execute_reply": "2025-05-26T16:50:35.488528Z"
    },
    "papermill": {
     "duration": 0.021754,
     "end_time": "2025-05-26T16:50:35.491431",
     "exception": false,
     "start_time": "2025-05-26T16:50:35.469677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RecursiveCharacterChunking(ChunkingStrategy):\n",
    "    \"\"\"Langchain's Recursive Character Text Splitter\"\"\"\n",
    "    \n",
    "    def chunk_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.config.chunk_size,\n",
    "            chunk_overlap=self.config.chunk_overlap,\n",
    "            separators=self.config.separators or [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        return splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "class FixedSizeChunking(ChunkingStrategy):\n",
    "    \"\"\"Simple fixed-size chunking with overlap\"\"\"\n",
    "    \n",
    "    def chunk_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        chunks = []\n",
    "        for doc in documents:\n",
    "            text = doc.page_content\n",
    "            start = 0\n",
    "            \n",
    "            while start < len(text):\n",
    "                end = min(start + self.config.chunk_size, len(text))\n",
    "                chunk_text = text[start:end]\n",
    "                \n",
    "                chunk = Document(\n",
    "                    page_content=chunk_text,\n",
    "                    metadata={**doc.metadata, \"chunk_start\": start, \"chunk_end\": end}\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "                \n",
    "                if end >= len(text):\n",
    "                    break\n",
    "                start = end - self.config.chunk_overlap\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "\n",
    "class SemanticChunking(ChunkingStrategy):\n",
    "    \"\"\"Sentence-based chunking that respects semantic boundaries\"\"\"\n",
    "    \n",
    "    def chunk_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        chunks = []\n",
    "        for doc in documents:\n",
    "            sentences = self._split_sentences(doc.page_content)\n",
    "            current_chunk = \"\"\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                # Check if adding sentence exceeds limit\n",
    "                if len(current_chunk) + len(sentence) > self.config.chunk_size and current_chunk:\n",
    "                    # Save current chunk\n",
    "                    chunk = Document(\n",
    "                        page_content=current_chunk.strip(),\n",
    "                        metadata=doc.metadata\n",
    "                    )\n",
    "                    chunks.append(chunk)\n",
    "                    \n",
    "                    # Start new chunk with overlap\n",
    "                    overlap_words = current_chunk.split()[-self.config.chunk_overlap//10:]\n",
    "                    current_chunk = \" \".join(overlap_words) + \" \" + sentence\n",
    "                else:\n",
    "                    current_chunk += \" \" + sentence\n",
    "            \n",
    "            # Add final chunk\n",
    "            if current_chunk.strip():\n",
    "                chunk = Document(\n",
    "                    page_content=current_chunk.strip(),\n",
    "                    metadata=doc.metadata\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _split_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Simple sentence splitting\"\"\"\n",
    "        import re\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        return [s.strip() for s in sentences if s.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "182b18f2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-26T16:50:35.506087Z",
     "iopub.status.busy": "2025-05-26T16:50:35.505782Z",
     "iopub.status.idle": "2025-05-26T16:50:35.542379Z",
     "shell.execute_reply": "2025-05-26T16:50:35.541234Z"
    },
    "papermill": {
     "duration": 0.046074,
     "end_time": "2025-05-26T16:50:35.544154",
     "exception": false,
     "start_time": "2025-05-26T16:50:35.498080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RAGChunkingEvaluator:\n",
    "    \"\"\"Main evaluation system for RAG chunking strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, openai_api_key: str, persist_directory: str = \"./chroma_db\"):\n",
    "        self.openai_api_key = openai_api_key\n",
    "        self.persist_directory = persist_directory\n",
    "        self.embeddings = OpenAIEmbeddings(api_key=openai_api_key)\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=openai_api_key)\n",
    "        self.strategies: Dict[str, ChunkingStrategy] = {}\n",
    "        self.results: List[EvaluationResult] = []\n",
    "        \n",
    "        # Initialize Chroma client\n",
    "        self.chroma_client = chromadb.PersistentClient(path=persist_directory)\n",
    "    \n",
    "    def register_strategy(self, strategy: ChunkingStrategy):\n",
    "        \"\"\"Register a chunking strategy for evaluation\"\"\"\n",
    "        self.strategies[strategy.name] = strategy\n",
    "        print(f\"Registered strategy: {strategy.name}\")\n",
    "    \n",
    "    def load_documents(self, file_paths: List[str]) -> List[Document]:\n",
    "        \"\"\"Load documents from file paths\"\"\"\n",
    "        documents = []\n",
    "        for path in file_paths:\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                doc = Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\"source\": path}\n",
    "                )\n",
    "                documents.append(doc)\n",
    "        return documents\n",
    "    \n",
    "    def create_test_questions(self, documents: List[Document], num_questions: int = 10) -> List[Dict[str, str]]:\n",
    "        \"\"\"Generate test questions from documents\"\"\"\n",
    "        questions = []\n",
    "        \n",
    "        # Sample text for question generation\n",
    "        sample_texts = []\n",
    "        for doc in documents[:3]:  # Use first 3 docs\n",
    "            words = doc.page_content.split()[:500]  # First 500 words\n",
    "            sample_texts.append(\" \".join(words))\n",
    "        \n",
    "        combined_text = \"\\n\\n\".join(sample_texts)\n",
    "        \n",
    "        prompt = f\"\"\"Based on the following text, generate {num_questions} diverse questions that can be answered using the information provided. \n",
    "        \n",
    "        Text:\n",
    "        {combined_text}\n",
    "        \n",
    "        Return only the questions, one per line, without numbering or additional text.\"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        question_lines = [q.strip() for q in response.content.split('\\n') if q.strip()]\n",
    "        \n",
    "        for i, question in enumerate(question_lines[:num_questions]):\n",
    "            questions.append({\n",
    "                \"question\": question,\n",
    "                \"question_id\": f\"q_{i+1}\"\n",
    "            })\n",
    "        \n",
    "        return questions\n",
    "    \n",
    "    def evaluate_strategy(self, \n",
    "                         strategy_name: str, \n",
    "                         documents: List[Document],\n",
    "                         test_questions: List[Dict[str, str]],\n",
    "                         top_k: int = 5) -> EvaluationResult:\n",
    "        \"\"\"Evaluate a single chunking strategy\"\"\"\n",
    "        \n",
    "        if strategy_name not in self.strategies:\n",
    "            raise ValueError(f\"Strategy {strategy_name} not registered\")\n",
    "        \n",
    "        strategy = self.strategies[strategy_name]\n",
    "        print(f\"\\nEvaluating strategy: {strategy_name}\")\n",
    "        \n",
    "        # 1. Chunk documents\n",
    "        chunks = strategy.chunk_documents(documents)\n",
    "        print(f\"Created {len(chunks)} chunks\")\n",
    "        \n",
    "        # 2. Create vector store\n",
    "        collection_name = f\"eval_{strategy_name}_{hash(str(time.time()))}\"\n",
    "        collection_name = collection_name.replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "        \n",
    "        vectorstore = Chroma(\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=self.embeddings,\n",
    "            client=self.chroma_client\n",
    "        )\n",
    "        \n",
    "        # Add chunks to vector store\n",
    "        vectorstore.add_documents(chunks)\n",
    "        \n",
    "        # 3. Evaluate with test questions\n",
    "        evaluation_data = []\n",
    "        \n",
    "        for q_data in tqdm(test_questions, desc=\"Processing questions\"):\n",
    "            question = q_data[\"question\"]\n",
    "            \n",
    "            # Retrieve context\n",
    "            start_time = time.time()\n",
    "            retrieved_docs = vectorstore.similarity_search(question, k=top_k)\n",
    "            retrieval_time = time.time() - start_time\n",
    "            \n",
    "            contexts = [doc.page_content for doc in retrieved_docs]\n",
    "            \n",
    "            # Generate answer\n",
    "            start_time = time.time()\n",
    "            context_text = \"\\n\\n\".join(contexts)\n",
    "            prompt = f\"Context:\\n{context_text}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "            \n",
    "            response = self.llm.invoke(prompt)\n",
    "            answer = response.content\n",
    "            generation_time = time.time() - start_time\n",
    "            \n",
    "            # Prepare for RAGAS\n",
    "            evaluation_data.append({\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"contexts\": contexts,\n",
    "                \"ground_truth\": answer,  # Using generated answer as proxy\n",
    "                \"retrieval_time\": retrieval_time,\n",
    "                \"generation_time\": generation_time\n",
    "            })\n",
    "        \n",
    "        # 4. Run RAGAS evaluation\n",
    "        dataset = Dataset.from_list(evaluation_data)\n",
    "        \n",
    "        metrics = [\n",
    "            answer_relevancy,\n",
    "            faithfulness,\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "            answer_correctness\n",
    "        ]\n",
    "        \n",
    "        ragas_results = evaluate(dataset, metrics=metrics)\n",
    "        \n",
    "        # 5. Calculate statistics\n",
    "        avg_chunk_size = np.mean([len(chunk.page_content) for chunk in chunks])\n",
    "        avg_retrieval_time = np.mean([d[\"retrieval_time\"] for d in evaluation_data])\n",
    "        avg_generation_time = np.mean([d[\"generation_time\"] for d in evaluation_data])\n",
    "        \n",
    "        # Calculate overall score (weighted average)\n",
    "        overall_score = (\n",
    "            ragas_results[\"answer_relevancy\"] * 0.25 +\n",
    "            ragas_results[\"faithfulness\"] * 0.25 +\n",
    "            ragas_results[\"context_precision\"] * 0.2 +\n",
    "            ragas_results[\"context_recall\"] * 0.2 +\n",
    "            ragas_results[\"answer_correctness\"] * 0.1\n",
    "        )\n",
    "        \n",
    "        result = EvaluationResult(\n",
    "            strategy_name=strategy_name,\n",
    "            chunk_count=len(chunks),\n",
    "            avg_chunk_size=avg_chunk_size,\n",
    "            retrieval_time=avg_retrieval_time,\n",
    "            generation_time=avg_generation_time,\n",
    "            answer_relevancy=ragas_results[\"answer_relevancy\"],\n",
    "            faithfulness=ragas_results[\"faithfulness\"],\n",
    "            context_precision=ragas_results[\"context_precision\"],\n",
    "            context_recall=ragas_results[\"context_recall\"],\n",
    "            answer_correctness=ragas_results[\"answer_correctness\"],\n",
    "            overall_score=overall_score\n",
    "        )\n",
    "        \n",
    "        # Cleanup\n",
    "        try:\n",
    "            self.chroma_client.delete_collection(collection_name)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def run_evaluation(self, \n",
    "                      documents: List[Document],\n",
    "                      test_questions: Optional[List[Dict[str, str]]] = None,\n",
    "                      num_questions: int = 10) -> List[EvaluationResult]:\n",
    "        \"\"\"Run evaluation on all registered strategies\"\"\"\n",
    "        \n",
    "        if not test_questions:\n",
    "            print(\"Generating test questions...\")\n",
    "            test_questions = self.create_test_questions(documents, num_questions)\n",
    "        \n",
    "        self.results = []\n",
    "        \n",
    "        for strategy_name in self.strategies:\n",
    "            try:\n",
    "                result = self.evaluate_strategy(strategy_name, documents, test_questions)\n",
    "                self.results.append(result)\n",
    "                print(f\"✓ Completed {strategy_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Failed {strategy_name}: {str(e)}\")\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def create_dashboard(self, save_path: str = \"rag_evaluation_dashboard.html\"):\n",
    "        \"\"\"Create interactive Plotly dashboard\"\"\"\n",
    "        \n",
    "        if not self.results:\n",
    "            print(\"No results to visualize. Run evaluation first.\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame([asdict(result) for result in self.results])\n",
    "        \n",
    "        # Create subplots\n",
    "        fig = make_subplots(\n",
    "            rows=3, cols=2,\n",
    "            subplot_titles=[\n",
    "                \"Overall Performance Comparison\",\n",
    "                \"Chunk Statistics\",\n",
    "                \"RAGAS Metrics Breakdown\",\n",
    "                \"Performance vs Chunk Size\",\n",
    "                \"Retrieval & Generation Time\",\n",
    "                \"Strategy Rankings\"\n",
    "            ],\n",
    "            specs=[\n",
    "                [{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "                [{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "                [{\"type\": \"bar\"}, {\"type\": \"table\"}]\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # 1. Overall Performance\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=df['strategy_name'],\n",
    "                y=df['overall_score'],\n",
    "                name=\"Overall Score\",\n",
    "                marker_color='lightblue'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # 2. Chunk Statistics\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df['chunk_count'],\n",
    "                y=df['avg_chunk_size'],\n",
    "                mode='markers+text',\n",
    "                text=df['strategy_name'],\n",
    "                textposition=\"top center\",\n",
    "                name=\"Chunk Stats\",\n",
    "                marker=dict(size=10, color='orange')\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # 3. RAGAS Metrics\n",
    "        metrics = ['answer_relevancy', 'faithfulness', 'context_precision', 'context_recall', 'answer_correctness']\n",
    "        for i, metric in enumerate(metrics):\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=df['strategy_name'],\n",
    "                    y=df[metric],\n",
    "                    name=metric.replace('_', ' ').title(),\n",
    "                    offsetgroup=i\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "        \n",
    "        # 4. Performance vs Chunk Size\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df['avg_chunk_size'],\n",
    "                y=df['overall_score'],\n",
    "                mode='markers+text',\n",
    "                text=df['strategy_name'],\n",
    "                textposition=\"top center\",\n",
    "                name=\"Performance vs Size\",\n",
    "                marker=dict(size=12, color='green')\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        # 5. Time Analysis\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=df['strategy_name'],\n",
    "                y=df['retrieval_time'],\n",
    "                name=\"Retrieval Time\",\n",
    "                marker_color='red',\n",
    "                offsetgroup=0\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=df['strategy_name'],\n",
    "                y=df['generation_time'],\n",
    "                name=\"Generation Time\",\n",
    "                marker_color='blue',\n",
    "                offsetgroup=1\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "        \n",
    "        # 6. Rankings Table\n",
    "        df_sorted = df.sort_values('overall_score', ascending=False)\n",
    "        df_sorted['rank'] = range(1, len(df_sorted) + 1)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Table(\n",
    "                header=dict(\n",
    "                    values=['Rank', 'Strategy', 'Overall Score', 'Best Metric'],\n",
    "                    fill_color='lightgray'\n",
    "                ),\n",
    "                cells=dict(\n",
    "                    values=[\n",
    "                        df_sorted['rank'],\n",
    "                        df_sorted['strategy_name'],\n",
    "                        [f\"{score:.3f}\" for score in df_sorted['overall_score']],\n",
    "                        [self._best_metric(row) for _, row in df_sorted.iterrows()]\n",
    "                    ]\n",
    "                )\n",
    "            ),\n",
    "            row=3, col=2\n",
    "        )\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            height=1200,\n",
    "            title=\"RAG Chunking Strategy Performance Dashboard\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        # Save dashboard\n",
    "        pyo.plot(fig, filename=save_path, auto_open=False)\n",
    "        print(f\"Dashboard saved to: {save_path}\")\n",
    "        return fig\n",
    "    \n",
    "    def _best_metric(self, row) -> str:\n",
    "        \"\"\"Find the best performing metric for a strategy\"\"\"\n",
    "        metrics = ['answer_relevancy', 'faithfulness', 'context_precision', 'context_recall', 'answer_correctness']\n",
    "        best_metric = max(metrics, key=lambda m: row[m])\n",
    "        return best_metric.replace('_', ' ').title()\n",
    "    \n",
    "    def save_results(self, filename: str = \"rag_evaluation_results.json\"):\n",
    "        \"\"\"Save evaluation results to JSON\"\"\"\n",
    "        results_dict = [asdict(result) for result in self.results]\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(results_dict, f, indent=2)\n",
    "        print(f\"Results saved to: {filename}\")\n",
    "    \n",
    "    def get_best_strategy(self) -> Optional[EvaluationResult]:\n",
    "        \"\"\"Get the best performing strategy\"\"\"\n",
    "        if not self.results:\n",
    "            return None\n",
    "        return max(self.results, key=lambda r: r.overall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79c06e80",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-26T16:50:35.560040Z",
     "iopub.status.busy": "2025-05-26T16:50:35.559728Z",
     "iopub.status.idle": "2025-05-26T16:50:35.567367Z",
     "shell.execute_reply": "2025-05-26T16:50:35.566115Z"
    },
    "papermill": {
     "duration": 0.017605,
     "end_time": "2025-05-26T16:50:35.569442",
     "exception": false,
     "start_time": "2025-05-26T16:50:35.551837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def demo_usage():\n",
    "    \"\"\"Demonstrate how to use the RAG Chunking Evaluator\"\"\"\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = RAGChunkingEvaluator(\n",
    "        openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "        persist_directory=\"./demo_chroma\"\n",
    "    )\n",
    "    \n",
    "    # Register strategies\n",
    "    evaluator.register_strategy(\n",
    "        RecursiveCharacterChunking(\n",
    "            ChunkingConfig(name=\"Recursive-1000\", chunk_size=1000, chunk_overlap=200)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    evaluator.register_strategy(\n",
    "        RecursiveCharacterChunking(\n",
    "            ChunkingConfig(name=\"Recursive-500\", chunk_size=500, chunk_overlap=100)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    evaluator.register_strategy(\n",
    "        FixedSizeChunking(\n",
    "            ChunkingConfig(name=\"Fixed-1000\", chunk_size=1000, chunk_overlap=200)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    evaluator.register_strategy(\n",
    "        SemanticChunking(\n",
    "            ChunkingConfig(name=\"Semantic-1000\", chunk_size=1000, chunk_overlap=50)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Load documents (replace with your document paths)\n",
    "    documents = evaluator.load_documents([\n",
    "        \"document1.txt\",\n",
    "        \"document2.txt\"\n",
    "    ])\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluator.run_evaluation(documents, num_questions=15)\n",
    "    \n",
    "    # Create dashboard\n",
    "    evaluator.create_dashboard(\"chunking_evaluation.html\")\n",
    "    \n",
    "    # Save results\n",
    "    evaluator.save_results(\"evaluation_results.json\")\n",
    "    \n",
    "    # Print best strategy\n",
    "    best = evaluator.get_best_strategy()\n",
    "    if best:\n",
    "        print(f\"\\nBest Strategy: {best.strategy_name}\")\n",
    "        print(f\"Overall Score: {best.overall_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7d118b8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-26T16:50:35.584022Z",
     "iopub.status.busy": "2025-05-26T16:50:35.583677Z",
     "iopub.status.idle": "2025-05-26T16:50:35.589223Z",
     "shell.execute_reply": "2025-05-26T16:50:35.588181Z"
    },
    "papermill": {
     "duration": 0.014558,
     "end_time": "2025-05-26T16:50:35.590959",
     "exception": false,
     "start_time": "2025-05-26T16:50:35.576401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Chunking Strategy Evaluator\n",
      "==================================================\n",
      "\n",
      "To use this system:\n",
      "1. Set your OpenAI API key\n",
      "2. Register chunking strategies\n",
      "3. Load your documents\n",
      "4. Run evaluation\n",
      "5. View dashboard\n",
      "\n",
      "See demo_usage() function for example.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"RAG Chunking Strategy Evaluator\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nTo use this system:\")\n",
    "    print(\"1. Set your OpenAI API key\")\n",
    "    print(\"2. Register chunking strategies\")\n",
    "    print(\"3. Load your documents\")\n",
    "    print(\"4. Run evaluation\")\n",
    "    print(\"5. View dashboard\")\n",
    "    print(\"\\nSee demo_usage() function for example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d2c8a91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T16:50:35.605671Z",
     "iopub.status.busy": "2025-05-26T16:50:35.604761Z",
     "iopub.status.idle": "2025-05-26T16:50:37.685970Z",
     "shell.execute_reply": "2025-05-26T16:50:37.683890Z"
    },
    "papermill": {
     "duration": 2.090342,
     "end_time": "2025-05-26T16:50:37.687901",
     "exception": false,
     "start_time": "2025-05-26T16:50:35.597559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "100  177k  100  177k    0     0   224k      0 --:--:-- --:--:-- --:--:--  224k\r\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "100  734k  100  734k    0     0   832k      0 --:--:-- --:--:-- --:--:--  833k\r\n"
     ]
    }
   ],
   "source": [
    "!curl https://www.gutenberg.org/files/46/46-0.txt > document1.txt\n",
    "!curl https://www.gutenberg.org/files/1342/1342-0.txt > document2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cc99a1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T16:50:37.703910Z",
     "iopub.status.busy": "2025-05-26T16:50:37.703585Z",
     "iopub.status.idle": "2025-05-26T16:50:37.709139Z",
     "shell.execute_reply": "2025-05-26T16:50:37.707987Z"
    },
    "papermill": {
     "duration": 0.016306,
     "end_time": "2025-05-26T16:50:37.711049",
     "exception": false,
     "start_time": "2025-05-26T16:50:37.694743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# demo_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38ec1889",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T16:50:37.727229Z",
     "iopub.status.busy": "2025-05-26T16:50:37.726854Z",
     "iopub.status.idle": "2025-05-26T16:50:39.347033Z",
     "shell.execute_reply": "2025-05-26T16:50:39.345886Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 1.631331,
     "end_time": "2025-05-26T16:50:39.349475",
     "exception": false,
     "start_time": "2025-05-26T16:50:37.718144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from dataclasses import asdict\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_individual_charts(results, output_dir=\"rag_charts\", cost_per_token=None, format='html'):\n",
    "    \"\"\"\n",
    "    Create individual RAG evaluation charts\n",
    "    \n",
    "    Args:\n",
    "        results: List of evaluation results OR path to JSON file\n",
    "        output_dir: Directory to save chart files\n",
    "        format: 'html' (recommended) or 'png'\n",
    "        cost_per_token: Dict with 'input' and 'output' token costs for cost analysis\n",
    "                       e.g., {'input': 0.00001, 'output': 0.00003}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data if results is a file path\n",
    "    if isinstance(results, str):\n",
    "        with open(results, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        data = results\n",
    "        \n",
    "    if not data:\n",
    "        print(\"No results provided\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    def convert_to_dict(result):\n",
    "        if isinstance(result, dict):\n",
    "            return result\n",
    "        elif hasattr(result, '__dict__'):\n",
    "            return vars(result)\n",
    "        elif hasattr(result, '_asdict'):\n",
    "            return result._asdict()\n",
    "        else:\n",
    "            try:\n",
    "                return asdict(result)\n",
    "            except TypeError:\n",
    "                raise TypeError(f\"Cannot convert result type {type(result)} to dict\")\n",
    "    \n",
    "    df = pd.DataFrame([convert_to_dict(result) for result in data])\n",
    "    \n",
    "    # Add cost analysis if token costs provided\n",
    "    if cost_per_token and 'input_tokens' in df.columns and 'output_tokens' in df.columns:\n",
    "        df['cost_per_query'] = (df['input_tokens'] * cost_per_token['input'] + \n",
    "                               df['output_tokens'] * cost_per_token['output'])\n",
    "        df['cost_per_performance'] = df['cost_per_query'] / df['overall_score']\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Color palette\n",
    "    colors = {\n",
    "        'primary': '#1f77b4',\n",
    "        'secondary': '#ff7f0e', \n",
    "        'success': '#2ca02c',\n",
    "        'danger': '#d62728',\n",
    "        'warning': '#ff9800',\n",
    "        'info': '#17a2b8',\n",
    "        'purple': '#9467bd',\n",
    "        'gray': '#7f7f7f'\n",
    "    }\n",
    "    \n",
    "    charts_created = []\n",
    "    file_ext = 'html' if format == 'html' else 'png'\n",
    "    \n",
    "    # Helper function to save charts\n",
    "    def save_chart(fig, filename_base):\n",
    "        filename = f\"{output_dir}/{filename_base}.{file_ext}\"\n",
    "        if format == 'html':\n",
    "            fig.write_html(filename)\n",
    "        else:\n",
    "            try:\n",
    "                # Try different export methods for PNG\n",
    "                fig.write_image(filename, width=1000, height=700, engine=\"kaleido\")\n",
    "            except Exception as e1:\n",
    "                try:\n",
    "                    fig.write_image(filename, width=1000, height=700, engine=\"orca\")\n",
    "                except Exception as e2:\n",
    "                    try:\n",
    "                        import plotly.io as pio\n",
    "                        pio.write_image(fig, filename, width=1000, height=700)\n",
    "                    except Exception as e3:\n",
    "                        print(f\"PNG export failed. Saving as HTML instead: {e3}\")\n",
    "                        filename = f\"{output_dir}/{filename_base}.html\"\n",
    "                        fig.write_html(filename)\n",
    "        return filename\n",
    "    \n",
    "    # Statistical significance testing\n",
    "    def perform_significance_tests():\n",
    "        \"\"\"Test if performance differences are statistically significant\"\"\"\n",
    "        strategies = df['strategy_name'].unique()\n",
    "        results = {}\n",
    "        \n",
    "        if len(strategies) < 2:\n",
    "            return results\n",
    "            \n",
    "        for i, strategy1 in enumerate(strategies):\n",
    "            for strategy2 in strategies[i+1:]:\n",
    "                score1 = df[df['strategy_name'] == strategy1]['overall_score'].values\n",
    "                score2 = df[df['strategy_name'] == strategy2]['overall_score'].values\n",
    "                \n",
    "                # Use Mann-Whitney U test (non-parametric) since we likely have small samples\n",
    "                try:\n",
    "                    statistic, p_value = mannwhitneyu(score1, score2, alternative='two-sided')\n",
    "                    results[f\"{strategy1} vs {strategy2}\"] = {\n",
    "                        'p_value': p_value,\n",
    "                        'significant': p_value < 0.05,\n",
    "                        'effect_size': abs(np.mean(score1) - np.mean(score2)) / np.sqrt((np.var(score1) + np.var(score2)) / 2)\n",
    "                    }\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "        return results\n",
    "    \n",
    "    sig_tests = perform_significance_tests()\n",
    "    \n",
    "    # 1. Performance Ranking with Statistical Significance\n",
    "    def create_performance_chart():\n",
    "        df_sorted = df.sort_values('overall_score', ascending=True)\n",
    "        \n",
    "        # Add error bars if multiple measurements per strategy\n",
    "        error_bars = []\n",
    "        for strategy in df_sorted['strategy_name']:\n",
    "            strategy_scores = df[df['strategy_name'] == strategy]['overall_score']\n",
    "            if len(strategy_scores) > 1:\n",
    "                error_bars.append(strategy_scores.std())\n",
    "            else:\n",
    "                error_bars.append(0)\n",
    "        \n",
    "        fig = go.Figure(go.Bar(\n",
    "            x=df_sorted['overall_score'],\n",
    "            y=df_sorted['strategy_name'],\n",
    "            orientation='h',\n",
    "            marker_color=colors['primary'],\n",
    "            text=[f\"{score:.3f}\" for score in df_sorted['overall_score']],\n",
    "            textposition='outside',\n",
    "            error_x=dict(type='data', array=error_bars, visible=True),\n",
    "            hovertemplate='<b>%{y}</b><br>Score: %{x:.3f}<br>Std Dev: %{error_x:.3f}<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        # Add significance annotations\n",
    "        annotations = []\n",
    "        if sig_tests:\n",
    "            top_strategy = df_sorted.iloc[-1]['strategy_name']\n",
    "            y_pos = len(df_sorted) - 1\n",
    "            \n",
    "            for comparison, test_result in sig_tests.items():\n",
    "                if top_strategy in comparison and test_result['significant']:\n",
    "                    annotations.append(\n",
    "                        dict(x=df_sorted.iloc[-1]['overall_score'], y=y_pos,\n",
    "                             text=f\"p<0.05*\", showarrow=True, arrowhead=2,\n",
    "                             arrowcolor='red', arrowsize=1, arrowwidth=2)\n",
    "                    )\n",
    "                    break\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': 'RAG Strategy Performance Ranking<br><sub>Error bars show standard deviation • * indicates statistical significance</sub>',\n",
    "                'x': 0.5,\n",
    "                'font': {'size': 20}\n",
    "            },\n",
    "            xaxis_title='Overall Performance Score',\n",
    "            yaxis_title='Chunking Strategy',\n",
    "            height=max(500, len(df) * 60),\n",
    "            width=1000,\n",
    "            margin=dict(l=200, r=150, t=120, b=80),\n",
    "            plot_bgcolor='white',\n",
    "            font=dict(size=14),\n",
    "            annotations=annotations\n",
    "        )\n",
    "        \n",
    "        fig.update_xaxes(showgrid=True, gridcolor='lightgray')\n",
    "        return save_chart(fig, \"01_performance_ranking\")\n",
    "    \n",
    "    # 2. Chunk Distribution with Performance Correlation\n",
    "    def create_chunk_analysis():\n",
    "        # Calculate correlation\n",
    "        chunk_perf_corr = df['chunk_count'].corr(df['overall_score'])\n",
    "        size_perf_corr = df['avg_chunk_size'].corr(df['overall_score'])\n",
    "        \n",
    "        fig = go.Figure(go.Scatter(\n",
    "            x=df['chunk_count'],\n",
    "            y=df['avg_chunk_size'],\n",
    "            mode='markers+text',\n",
    "            text=df['strategy_name'],\n",
    "            textposition=\"top center\",\n",
    "            marker=dict(\n",
    "                size=df['overall_score'] * 40,\n",
    "                color=df['overall_score'],\n",
    "                colorscale='Viridis',\n",
    "                showscale=True,\n",
    "                colorbar=dict(\n",
    "                    title=\"Performance Score\",\n",
    "                    titleside=\"right\",\n",
    "                    len=0.7\n",
    "                ),\n",
    "                line=dict(width=2, color='white'),\n",
    "                opacity=0.8\n",
    "            ),\n",
    "            hovertemplate='<b>%{text}</b><br>' +\n",
    "                         'Chunks: %{x}<br>' +\n",
    "                         'Avg Size: %{y:.0f} chars<br>' +\n",
    "                         'Score: %{marker.color:.3f}<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': f'Chunk Distribution vs Performance<br><sub>Bubble size = performance • Chunk-Perf correlation: {chunk_perf_corr:.3f} • Size-Perf correlation: {size_perf_corr:.3f}</sub>',\n",
    "                'x': 0.5,\n",
    "                'font': {'size': 20}\n",
    "            },\n",
    "            xaxis_title='Number of Chunks',\n",
    "            yaxis_title='Average Chunk Size (characters)',\n",
    "            height=700,\n",
    "            width=1000,\n",
    "            plot_bgcolor='white',\n",
    "            font=dict(size=14)\n",
    "        )\n",
    "        \n",
    "        fig.update_xaxes(showgrid=True, gridcolor='lightgray')\n",
    "        fig.update_yaxes(showgrid=True, gridcolor='lightgray')\n",
    "        \n",
    "        return save_chart(fig, \"02_chunk_distribution\")\n",
    "    \n",
    "    # 3. RAGAS Metrics Radar Chart\n",
    "    def create_metrics_radar():\n",
    "        metrics = ['answer_relevancy', 'faithfulness', 'context_precision', 'context_recall', 'answer_correctness']\n",
    "        metric_labels = [m.replace('_', ' ').title() for m in metrics]\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        colors_list = [colors['primary'], colors['secondary'], colors['success'], colors['danger'], colors['warning'], colors['purple']]\n",
    "        \n",
    "        for i, (_, row) in enumerate(df.iterrows()):\n",
    "            values = [row[metric] for metric in metrics]\n",
    "            values += [values[0]]  # Close the radar chart\n",
    "            \n",
    "            fig.add_trace(go.Scatterpolar(\n",
    "                r=values,\n",
    "                theta=metric_labels + [metric_labels[0]],\n",
    "                fill='toself',\n",
    "                name=row['strategy_name'],\n",
    "                line_color=colors_list[i % len(colors_list)],\n",
    "                opacity=0.6\n",
    "            ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            polar=dict(\n",
    "                radialaxis=dict(\n",
    "                    visible=True,\n",
    "                    range=[0, 1],\n",
    "                    tickformat='.2f'\n",
    "                )\n",
    "            ),\n",
    "            title={\n",
    "                'text': 'RAGAS Metrics Comparison<br><sub>All metrics normalized to 0-1 scale</sub>',\n",
    "                'x': 0.5,\n",
    "                'font': {'size': 20}\n",
    "            },\n",
    "            height=700,\n",
    "            width=1000,\n",
    "            font=dict(size=14),\n",
    "            legend=dict(\n",
    "                orientation=\"v\",\n",
    "                yanchor=\"middle\",\n",
    "                y=0.5,\n",
    "                xanchor=\"left\",\n",
    "                x=1.05\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return save_chart(fig, \"03_metrics_radar\")\n",
    "    \n",
    "    # 4. Performance vs Chunk Size with Trend Analysis\n",
    "    def create_size_performance():\n",
    "        fig = go.Figure(go.Scatter(\n",
    "            x=df['avg_chunk_size'],\n",
    "            y=df['overall_score'],\n",
    "            mode='markers+text',\n",
    "            text=df['strategy_name'],\n",
    "            textposition=\"top center\",\n",
    "            marker=dict(\n",
    "                size=20,\n",
    "                color=colors['secondary'],\n",
    "                line=dict(width=2, color='white')\n",
    "            ),\n",
    "            hovertemplate='<b>%{text}</b><br>' +\n",
    "                         'Chunk Size: %{x:.0f} chars<br>' +\n",
    "                         'Performance: %{y:.3f}<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        # Add trendline with confidence interval\n",
    "        if len(df) > 2:\n",
    "            z = np.polyfit(df['avg_chunk_size'], df['overall_score'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            \n",
    "            x_trend = np.linspace(df['avg_chunk_size'].min(), df['avg_chunk_size'].max(), 100)\n",
    "            y_trend = p(x_trend)\n",
    "            \n",
    "            # Calculate R-squared\n",
    "            y_mean = np.mean(df['overall_score'])\n",
    "            ss_tot = np.sum((df['overall_score'] - y_mean) ** 2)\n",
    "            ss_res = np.sum((df['overall_score'] - p(df['avg_chunk_size'])) ** 2)\n",
    "            r_squared = 1 - (ss_res / ss_tot)\n",
    "            \n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=x_trend,\n",
    "                y=y_trend,\n",
    "                mode='lines',\n",
    "                name=f'Trend (R²={r_squared:.3f})',\n",
    "                line=dict(color=colors['danger'], dash='dash', width=3),\n",
    "                hovertemplate='Trendline<extra></extra>'\n",
    "            ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': 'Chunk Size vs Performance<br><sub>Identifying optimal chunk size ranges</sub>',\n",
    "                'x': 0.5,\n",
    "                'font': {'size': 20}\n",
    "            },\n",
    "            xaxis_title='Average Chunk Size (characters)',\n",
    "            yaxis_title='Overall Performance Score',\n",
    "            height=700,\n",
    "            width=1000,\n",
    "            plot_bgcolor='white',\n",
    "            font=dict(size=14),\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        fig.update_xaxes(showgrid=True, gridcolor='lightgray')\n",
    "        fig.update_yaxes(showgrid=True, gridcolor='lightgray')\n",
    "        \n",
    "        return save_chart(fig, \"04_size_vs_performance\")\n",
    "    \n",
    "    # 5. Processing Time Analysis\n",
    "    def create_time_analysis():\n",
    "        df_sorted = df.sort_values('retrieval_time', ascending=True)\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Total time for sorting\n",
    "        df_sorted['total_time'] = df_sorted['retrieval_time'] + df_sorted['generation_time']\n",
    "        \n",
    "        fig.add_trace(go.Bar(\n",
    "            y=df_sorted['strategy_name'],\n",
    "            x=df_sorted['retrieval_time'],\n",
    "            orientation='h',\n",
    "            name='Retrieval Time',\n",
    "            marker_color=colors['info'],\n",
    "            text=[f\"{t:.2f}s\" for t in df_sorted['retrieval_time']],\n",
    "            textposition='inside',\n",
    "            hovertemplate='<b>%{y}</b><br>Retrieval: %{x:.3f}s<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        fig.add_trace(go.Bar(\n",
    "            y=df_sorted['strategy_name'],\n",
    "            x=df_sorted['generation_time'],\n",
    "            orientation='h',\n",
    "            name='Generation Time',\n",
    "            marker_color=colors['warning'],\n",
    "            text=[f\"{t:.2f}s\" for t in df_sorted['generation_time']],\n",
    "            textposition='inside',\n",
    "            hovertemplate='<b>%{y}</b><br>Generation: %{x:.3f}s<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': 'Processing Time Breakdown<br><sub>Lower times indicate faster processing</sub>',\n",
    "                'x': 0.5,\n",
    "                'font': {'size': 20}\n",
    "            },\n",
    "            xaxis_title='Time (seconds)',\n",
    "            yaxis_title='Chunking Strategy',\n",
    "            height=max(500, len(df) * 70),\n",
    "            width=1000,\n",
    "            margin=dict(l=200, r=150, t=120, b=80),\n",
    "            plot_bgcolor='white',\n",
    "            font=dict(size=14),\n",
    "            barmode='group',\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=1.02,\n",
    "                xanchor=\"right\",\n",
    "                x=1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.update_xaxes(showgrid=True, gridcolor='lightgray')\n",
    "        \n",
    "        return save_chart(fig, \"05_processing_time\")\n",
    "    \n",
    "    # 6. Cost Analysis (if cost data available)\n",
    "    def create_cost_analysis():\n",
    "        if 'cost_per_query' not in df.columns:\n",
    "            return None\n",
    "            \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Cost per query\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df['cost_per_query'],\n",
    "            y=df['overall_score'],\n",
    "            mode='markers+text',\n",
    "            text=df['strategy_name'],\n",
    "            textposition=\"top center\",\n",
    "            name='Cost vs Performance',\n",
    "            marker=dict(\n",
    "                size=20,\n",
    "                color=colors['success'],\n",
    "                line=dict(width=2, color='white')\n",
    "            ),\n",
    "            hovertemplate='<b>%{text}</b><br>' +\n",
    "                         'Cost per Query: $%{x:.4f}<br>' +\n",
    "                         'Performance: %{y:.3f}<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        # Add Pareto frontier (efficiency frontier)\n",
    "        df_sorted = df.sort_values('cost_per_query')\n",
    "        pareto_points = []\n",
    "        max_performance = 0\n",
    "        \n",
    "        for _, row in df_sorted.iterrows():\n",
    "            if row['overall_score'] > max_performance:\n",
    "                pareto_points.append(row)\n",
    "                max_performance = row['overall_score']\n",
    "        \n",
    "        if len(pareto_points) > 1:\n",
    "            pareto_df = pd.DataFrame(pareto_points)\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=pareto_df['cost_per_query'],\n",
    "                y=pareto_df['overall_score'],\n",
    "                mode='lines',\n",
    "                name='Efficiency Frontier',\n",
    "                line=dict(color=colors['danger'], dash='dot', width=3),\n",
    "                hovertemplate='Pareto Frontier<extra></extra>'\n",
    "            ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': 'Cost vs Performance Analysis<br><sub>Pareto frontier shows most efficient strategies</sub>',\n",
    "                'x': 0.5,\n",
    "                'font': {'size': 20}\n",
    "            },\n",
    "            xaxis_title='Cost per Query ($)',\n",
    "            yaxis_title='Overall Performance Score',\n",
    "            height=700,\n",
    "            width=1000,\n",
    "            plot_bgcolor='white',\n",
    "            font=dict(size=14),\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        fig.update_xaxes(showgrid=True, gridcolor='lightgray')\n",
    "        fig.update_yaxes(showgrid=True, gridcolor='lightgray')\n",
    "        \n",
    "        return save_chart(fig, \"06_cost_analysis\")\n",
    "    \n",
    "    # 7. Statistical Significance Summary\n",
    "    def create_significance_chart():\n",
    "        if not sig_tests:\n",
    "            return None\n",
    "            \n",
    "        # Create significance matrix\n",
    "        strategies = df['strategy_name'].unique()\n",
    "        sig_matrix = np.zeros((len(strategies), len(strategies)))\n",
    "        \n",
    "        for i, strategy1 in enumerate(strategies):\n",
    "            for j, strategy2 in enumerate(strategies):\n",
    "                if i != j:\n",
    "                    comparison = f\"{strategy1} vs {strategy2}\"\n",
    "                    reverse_comparison = f\"{strategy2} vs {strategy1}\"\n",
    "                    \n",
    "                    if comparison in sig_tests:\n",
    "                        sig_matrix[i, j] = 1 if sig_tests[comparison]['significant'] else 0.5\n",
    "                    elif reverse_comparison in sig_tests:\n",
    "                        sig_matrix[i, j] = 1 if sig_tests[reverse_comparison]['significant'] else 0.5\n",
    "        \n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=sig_matrix,\n",
    "            x=strategies,\n",
    "            y=strategies,\n",
    "            colorscale=[[0, 'white'], [0.5, 'yellow'], [1, 'red']],\n",
    "            showscale=True,\n",
    "            colorbar=dict(\n",
    "                title=\"Significance\",\n",
    "                tickvals=[0, 0.5, 1],\n",
    "                ticktext=['No Test', 'Not Significant', 'Significant (p<0.05)']\n",
    "            ),\n",
    "            hovertemplate='<b>%{y} vs %{x}</b><br>Significance: %{z}<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': 'Statistical Significance Matrix<br><sub>Red indicates significant performance differences (p<0.05)</sub>',\n",
    "                'x': 0.5,\n",
    "                'font': {'size': 20}\n",
    "            },\n",
    "            xaxis_title='Strategy',\n",
    "            yaxis_title='Strategy',\n",
    "            height=600,\n",
    "            width=1000,\n",
    "            font=dict(size=14)\n",
    "        )\n",
    "        \n",
    "        return save_chart(fig, \"07_significance_matrix\")\n",
    "    \n",
    "    # Create all charts\n",
    "    try:\n",
    "        charts_created.append(create_performance_chart())\n",
    "        charts_created.append(create_chunk_analysis())\n",
    "        charts_created.append(create_metrics_radar())\n",
    "        charts_created.append(create_size_performance())\n",
    "        charts_created.append(create_time_analysis())\n",
    "        \n",
    "        cost_chart = create_cost_analysis()\n",
    "        if cost_chart:\n",
    "            charts_created.append(cost_chart)\n",
    "            \n",
    "        sig_chart = create_significance_chart()\n",
    "        if sig_chart:\n",
    "            charts_created.append(sig_chart)\n",
    "        \n",
    "        # Filter out None values\n",
    "        charts_created = [chart for chart in charts_created if chart is not None]\n",
    "        \n",
    "        print(f\"Created {len(charts_created)} {format.upper()} charts in '{output_dir}/':\")\n",
    "        for chart in charts_created:\n",
    "            print(f\"  - {os.path.basename(chart)}\")\n",
    "            \n",
    "        # Print statistical summary\n",
    "        if sig_tests:\n",
    "            print(f\"\\nStatistical Significance Summary:\")\n",
    "            significant_pairs = sum(1 for test in sig_tests.values() if test['significant'])\n",
    "            print(f\"  - {significant_pairs}/{len(sig_tests)} comparisons show significant differences\")\n",
    "            \n",
    "        return charts_created\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating charts: {e}\")\n",
    "        return None\n",
    "\n",
    "# Usage examples:\n",
    "# HTML format (recommended):\n",
    "# charts = create_individual_charts(\"/path/to/results.json\", format='html')\n",
    "\n",
    "# PNG format (if kaleido works):\n",
    "# charts = create_individual_charts(\"/path/to/results.json\", format='png')\n",
    "\n",
    "# With cost analysis:\n",
    "# cost_config = {'input': 0.00001, 'output': 0.00003}\n",
    "# charts = create_individual_charts(\"/path/to/results.json\", \n",
    "#                                 cost_per_token=cost_config, format='html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a81d277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T16:50:39.365767Z",
     "iopub.status.busy": "2025-05-26T16:50:39.365114Z",
     "iopub.status.idle": "2025-05-26T16:50:40.335381Z",
     "shell.execute_reply": "2025-05-26T16:50:40.334074Z"
    },
    "papermill": {
     "duration": 0.980251,
     "end_time": "2025-05-26T16:50:40.337195",
     "exception": false,
     "start_time": "2025-05-26T16:50:39.356944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 6 HTML charts in 'rag_charts/':\n",
      "  - 01_performance_ranking.html\n",
      "  - 02_chunk_distribution.html\n",
      "  - 03_metrics_radar.html\n",
      "  - 04_size_vs_performance.html\n",
      "  - 05_processing_time.html\n",
      "  - 07_significance_matrix.html\n",
      "\n",
      "Statistical Significance Summary:\n",
      "  - 0/6 comparisons show significant differences\n"
     ]
    }
   ],
   "source": [
    " charts = create_individual_charts(\"/kaggle/input/rag-evaluation-restults-json/evaluation_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7519543,
     "sourceId": 11959138,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 79.574749,
   "end_time": "2025-05-26T16:50:41.673950",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-26T16:49:22.099201",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
